{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8242c49-9a8d-46c4-ab58-76be76cca64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import time\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('nomic-ai/nomic-embed-text-v1', trust_remote_code=True)\n",
    "model.eval()\n",
    "_ = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dbeb3f-0e5c-4a15-bd9a-a0964bea7f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_texts(passages):\n",
    "    encoded_input = tokenizer(passages, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    \n",
    "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6eacf7-f5dc-4bd3-8e75-c0067387ebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(veca, vecb):\n",
    "    maga, magb = 0, 0\n",
    "    dotprod = 0\n",
    "    for a,b in zip(veca, vecb):\n",
    "        maga += a*a\n",
    "        magb += b*b\n",
    "        dotprod += a*b\n",
    "    mag = (maga*magb)**0.5\n",
    "    return dotprod/mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d68b99e-b10e-4f56-91fc-f4531661e79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= embed_texts([\"My arm really hurts!\", \"my arm is absolutely killing me.\"])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89b3b3a-f2a4-4eaf-91e3-bd9365704cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity(x[0], x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed0045-8428-41d1-892c-dcf65fedd062",
   "metadata": {},
   "source": [
    "# Exciting! Now let's do something useful..\n",
    "\n",
    "For the sake of this blog post, I thought it would be fun to create a \"prick detector\".\n",
    "\n",
    "unfriendly, prick, fuckhead, asshole, twat\n",
    "\"Generate 30 messages from someone who is being an asshole.\"}\n",
    "\n",
    "\n",
    "## Making FakeData with Ollama\n",
    "https://ollama.com/blog/openai-compatibility\n",
    "\n",
    "I use mistral-openorca as it's nicely been uncensored for us, so it can create mean messages!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24d6369-e25a-463c-b51b-1c9202b4a8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# connect to ollama local API\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused\n",
    ")\n",
    "\n",
    "def get_completion(messages):\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"mistral-openorca\",\n",
    "      messages=messages\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def unpack(resp):\n",
    "    return resp.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c2a112-0339-452e-9fac-e3f1acb8ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we do a \"one-shot\" which gives the LLM a hint at what it should output with an example.\n",
    "# This tends to make our output conform better to what we want, and reduces the risk of hallucination.\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant, which generates fake messages.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Generate three messages from someone who is being helpful.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"1. I'd love to help! Just tell me what you need and I'll get right to it.\\n2. No problem at all mate, just send me the files and I'll start editing them, just to confirm, you do want me to check only for typos and grammatical errors?\\n3. Absolutely, I'll just get the car from the garage, I'd be more than happy to drive you to the hospital.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Generate 30 messages from someone who is being toxic.\"}\n",
    "    ]\n",
    "\n",
    "start = time.time()\n",
    "response = get_completion(messages)\n",
    "stop = time.time()\n",
    "elapsed = stop-start\n",
    "print(round(response.usage.total_tokens/elapsed,2), \"tokens per second\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef2047e-39e3-46f7-b78d-c39436ece4a7",
   "metadata": {},
   "source": [
    "So we can generate fakedata, how very exciting!\n",
    "At this point \n",
    "\n",
    "## Let's Create a Classifier!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b7c52-51b1-4ab3-b0db-9695533d6f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example with streaming:\n",
    "\n",
    "def get_completion_stream(messages):\n",
    "    \"\"\"prints a streaming example\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"mistral-openorca\",\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        n=1,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    text_out = \"\"\n",
    "    for chunk in response:\n",
    "        t = chunk.choices[0].delta.content\n",
    "        text_out += t\n",
    "        print(t, end='')\n",
    "\n",
    "    return text_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2441ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_completion_stream(messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c5cfe9-0d36-4f73-9baa-97b17f1d6e66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
